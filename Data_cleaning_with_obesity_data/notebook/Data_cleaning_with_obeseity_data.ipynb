{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning with Obeseity data \n",
    "\n",
    "Data cleaning is such an integral part of data analysis.Unlike on Kaggle,almost all data you see in the real world would be dirty and messy. Some even say data cleaning would take 80% of data analysis time.\n",
    "\n",
    "The very fisrt step of any given data analysis project would be getting to know your data especially when you are dealing a messy one.\n",
    "\n",
    "So, lets clean this messy data to start our analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and Know Your Data\n",
    "\n",
    "Take a look at our data as below, the data is quite obscure,it's hard to understand for a human,not mentioned for a computer.in this kind of situation,you have ways to get acquaintance with your data as follows:\n",
    "\n",
    "1. Go to the data source page [WHO OBESITY DATA](https://apps.who.int/gho/data/node.main.A900A?lang=en)\n",
    "2. If solution 1 doesn not work or hard to do, you can always go to ask data curator directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why and how the data is messy?\n",
    "\n",
    "In most cases, data is collected by human or machines,a tiny glich would cause a long strip of bad data.if the data is collected by human, then it is a big chance that it would be messy. data can be dirty in many different ways,but mostly fall into those categories :\n",
    "\n",
    "1. Missing data : like NAN\n",
    "2. Validity of data : like 2016.1 / 2016.2 in the column\n",
    "3. Outliers : like if a BMI entry is greater than 100\n",
    "4. Consistency of data : the unit of every entry is not the same\n",
    "5. Correctness of data: we are not gonna go through this ,but it is an important part of doing analysis in bussiness world, basically you need external data source or database to cross check the data in your hand because as we always say:\n",
    "\n",
    "You dont know what you dont know\n",
    "\n",
    "6. Data is in wide form not in long form : we are gonna go deeper about this one.\n",
    "\n",
    "Are you ready, it's time to get our hands dirt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## long form VS wide form\n",
    "\n",
    "The original data we have here is in wide form which means the form is very wide literally.\n",
    "\n",
    "The .1 .2 in year number stand for gender, we gonna fix that later.\n",
    "\n",
    "Wide data is not easy to analyze or stored effectively in computer, so we want to change it as soon as we can. go to read this tidy-data if you want to know more.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rename the columns appropriately and unpivot the data in the desirable format using pandas melt()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correct the format\n",
    "\n",
    "1. we will drop the first 3 row since its actually headers in the original forms.\n",
    "2. correct year value\n",
    "3. correct the gender value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## correct the BMI value columns\n",
    "\n",
    "From the webpage of WHO we can know that the values in [] are actually estimation intervel，so we need to seperate them into 3 columns\n",
    "\n",
    "you can use str.matach() or str.findall() with regular expression to extract float number in this field,but we are gonna use str.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check validity of all columns or fields\n",
    "We now have a pretty clean data compared to the one we just got. but our job is still not done yet. we need to go through every columns or fields to make sure the data is reletively correct.\n",
    "\n",
    "**Country columns**\n",
    "\n",
    "### What we know：\n",
    "\n",
    "There is a country named country which need to be fixed\n",
    "\n",
    "There are Nones in country column which need to be fixed\n",
    "\n",
    "We have\n",
    "\n",
    "### What we do：\n",
    "\n",
    "We gonna drop those entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BMI \\ BMI_upper_esti and BMI_lower_esti columns\n",
    "\n",
    "### What we know：\n",
    "\n",
    "1. 4 contries have no BMI data which are Monaca,Sudan,South Sudan and San Marino,hence they dont have estimations.\n",
    "2. We have 191 countries that do have BMI data and each of them has 126 entries.\n",
    "3. The descriptive statistics of BMI data seems OK, no outliers.\n",
    "\n",
    "### What we do：\n",
    "\n",
    "1. We gonna create a new dataframe without those 4 countries to analyze.\n",
    "2. We gonna change the data type of BMI and estimations to float."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity (15 minutes)\n",
    "\n",
    "## Visualization and EDA\n",
    "Before you doing any EDA, come up with some questions first. Question orientated is always a good way to explore a set of data, you could easily fall into rabbit holes you enconter along the process otherwise.\n",
    "\n",
    "What question we could possibly answer through this data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
