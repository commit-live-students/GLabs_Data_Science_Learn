{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Challenge_in_ML_Class_imbalance.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_VcE3LKBrOp",
        "colab_type": "text"
      },
      "source": [
        "## Credit Delinquency Prediction\n",
        "\n",
        "\n",
        "### Problem statement\n",
        "\n",
        "Delinquency describes something or someone who fails to accomplish that which is required by law, duty, or contractual agreement, such as the failure to make a required payment or perform a particular action.\n",
        "\n",
        "Credit scoring algorithms, which makes a guess at the probability of default, are the method banks use to determine whether or not a loan should be granted. This use-case requires learners to improve on the state of the art in credit scoring, by predicting the probability that somebody will experience financial delinquency in the next two years.\n",
        "\n",
        "### Dataset description\n",
        "The dataset consists of 150000 records and 11 features. Below are the 11 features and their descriptions.\n",
        "\n",
        "|Feature|Description|\n",
        "|-----|-----|\n",
        "|SeriousDlqin2yrs|Person experienced 90 days past due delinquency or worse|\n",
        "|RevolvingUtilizationOfUnsecuredLines| Total balance on credit cards and personal lines of credit|\n",
        "|age| Age of borrower in years|\n",
        "|NumberOfTime30-59DaysPastDueNotWorse| Number of times borrower has been 30-59 days past due but no worse in the last 2 years|\n",
        "|DebtRatio| Monthly debt payments, alimony,living costs divided by monthy gross income|\n",
        "|MonthlyIncome|Monthly Income|\n",
        "|NumberOfOpenCreditLinesAndLoans| Number of Open loans (installment like car loan or mortgage) and Lines of credit|\n",
        "|NumberOfTimes90DaysLate|Number of times borrower has been 90 days or more past due|\n",
        "|NumberRealEstateLoansOrLines| Number of mortgage and real estate loans including home equity lines of credit|\n",
        "|NumberOfTime60-89DaysPastDueNotWorse| Number of times borrower has been 60-89 days past due but no worse in the last 2 years|\n",
        "|NumberOfDependents|Number of dependents in family excluding themselves|"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SsROpnfqB5JD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dlPBOq8jB7Wq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.pipeline import make_pipeline\n",
        "from pylab import rcParams\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score, recall_score, confusion_matrix,classification_report\n",
        "from sklearn.metrics import f1_score, roc_auc_score, roc_curve\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tc6I0ufcCEVQ",
        "colab_type": "text"
      },
      "source": [
        "### Task 1 :Load the data and get an overview of the data using `.describe()` and `.info()` method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwvDe0AHCLK6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Up0vC58CV1p",
        "colab_type": "text"
      },
      "source": [
        "### Task 2 : Check for the skewness in the variables in `NumberOfDependents`  and `MonthlyIncome` by plotting a histogram."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9OgVEwMbCeD8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OiCwWibCe4p",
        "colab_type": "text"
      },
      "source": [
        "### Task 3 :There is skewness in the feature `NumberOfDependents`. So let's replace the null values in this feature with the median and let's do the same for the feature `MonthlyIncome`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQXSC99vCjbL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CpQozNmbClpv",
        "colab_type": "text"
      },
      "source": [
        "### Task 4: Check for the distribution of the target variable using a `countplot()`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ncD2YUSSCpuK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1Za8j2wCqkh",
        "colab_type": "text"
      },
      "source": [
        "### Task 5 : Seperate the predictors and the target and split the data into training set and testing set. Keep the `test_size = 0.2` and the `random_state=42` "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UvWMcD5wCv8v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1etV0d6Cxxi",
        "colab_type": "text"
      },
      "source": [
        "### Task 6 : For a better method of inference, let's check for the correlation between different features by plotting a heatmap. The basic rule of feature selection is that we need to select features which are highly correlated to the dependent variable and also not highly correlated with each other as they show the same trend. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wzgh0NVC144",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EnMCMVsrC237",
        "colab_type": "text"
      },
      "source": [
        "### Task 7 : We can see that the features `NumberOfTime60-89DaysPastDueNotWorse` is highly correlated along with the features `NumberOfTime30-59DaysPastDueNotWorse` and `NumberOfTimes90DaysLate`. So let's drop the features `NumberOfTime60-89DaysPastDueNotWorse` and `NumberOfTime30-59DaysPastDueNotWorse` from the train as well as the test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6zZr38WC9ol",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GotgjdZ6C-vJ",
        "colab_type": "text"
      },
      "source": [
        "### Task 8 : Fit a vanilla Logistic Regression model on the training set and predict on the test set and plot the confusion matrix, accuracy, precision, recall and F1_score for the predicted model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6vXkbRnHDCBq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bd_yFJR7DCze",
        "colab_type": "text"
      },
      "source": [
        "### Task 9 : Set the parameter `class_weight=balanced` inside Logistic Regression and check for the metrics calculated above and also the confusion matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-L-k-lPRDKue",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMGtuasIDMJb",
        "colab_type": "text"
      },
      "source": [
        "### Task 10 : Perform Random Undersampling on the train data and then fit a Logistic regression model on this undersampled data and then predict on the test data and calculate the precision, recall, accuracy, f1-score and the confusion matrix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ymC2i4c5DRaF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6_fBA4pDaJF",
        "colab_type": "text"
      },
      "source": [
        "### Task 11 : Perform Tomek Undersampling on the train data and then fit a Logistic regression model on this undersampled data and then predict on the test data and calculate the precision, recall, accuracy, f1-score and the confusion matrix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lEyaefUMDaw9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbDSwqZeDcZs",
        "colab_type": "text"
      },
      "source": [
        "### Task 12 : Perform Random Oversampling on the train data and then fit a Logistic regression model on this undersampled data and then predict on the test data and calculate the precision, recall, accuracy, f1-score and the confusion matrix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aeNvn4hJDgYV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jgCnrtwZDhcJ",
        "colab_type": "text"
      },
      "source": [
        "### Task 13 : Perform SMOTE on the train data and then fit a Logistic regression model on this undersampled data and then predict on the test data and calculate the precision, recall, accuracy, f1-score and the confusion matrix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8ZHyHGZDlwV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}