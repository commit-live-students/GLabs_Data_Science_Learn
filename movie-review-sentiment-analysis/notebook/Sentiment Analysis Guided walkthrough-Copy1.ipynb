{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Movie reviews sentiment analysis challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "Sentiment Analysis is the most common text classification tool that analyses an incoming message and tells whether the underlying sentiment is positive, negative or neutral. You can input a sentence of your choice and gauge the underlying sentiment by playing with the demo [here](https://www.paralleldots.com/sentiment-analysis).\n",
    "\n",
    "We have a dataset which has a couple of columns, one of them being a review (or the description). The idea is to determine the sentiment from the review text. Essentially, that would consist of figuring out how positive or negative the sentiment is. Which means, this would be more or less of a regression problem than a classification one. \n",
    "\n",
    "\n",
    "There are 2 files, a `train.tsv` and a `test.tsv`. We will train our models on the `train.tsv` dataset. First we will split the `train.tsv` dataset into training data and testing data to determine the model accuracy, take the best model and then test it on our `test.tsv`. It will be imperative to combine both train and test tsvs onto a single file, just for the purpose of Vectorizing them. The model will not be trained on the complete set!!\n",
    "\n",
    "\n",
    "# About the dataset\n",
    "\n",
    "\n",
    "The snapshot of the data you will be working on:\n",
    "\n",
    "<img src=\"movie.png\">\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Why solve it ?\n",
    "\n",
    "Solving it will help you apply the following skills:\n",
    "\n",
    "- Text preprocessing techniques like\n",
    "    - Tokenization \n",
    "    - Stopword removal\n",
    "    - Count vectorizer\n",
    "    - Tf-idf vectorizer\n",
    "\n",
    "- Implementation of \n",
    "    - Random forest\n",
    "    - Naive Bayes\n",
    "    - Linear SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Preprocess the data\n",
    "\n",
    "Transforming text into something an algorithm can digest it a complicated process. We cannot feed the data as it is, some preprocessing needs to be done. In this task we will be doing some preprocessing to convert our data in a form that we can feed our model with.\n",
    "\n",
    "## Instructions\n",
    "* Load the data which is in a `.tsv` format and convert it to a dataframe using `pd.DataFrame.from_csv()` with parameters `path=path_train` and `sep=\"\\t\"` for training data and store it in a variable `df_train`. Similarly load the test data and store it in a variable `df_test`\n",
    "\n",
    "* Concat the column `Phrase` from `df_train` and `df_test` data and store it in a new dataframe `phrases`\n",
    "\n",
    "* Convert the `Phrase` column to lower case and assign it to a pandas series called `all_text`\n",
    "\n",
    "## Hints\n",
    "* Use `phrases = pd.concat([df_train[[\"Phrase\"]], df_test[[\"Phrase\"]]])` to concat the `Phrase` column from train and test dataframe\n",
    "\n",
    "## Test case\n",
    "* Variable declration `df_train`,`df_test`,`phrases` and `all_text`\n",
    "    - df_train.shape==(156060, 3)\n",
    "    - df_test.shape==(66292, 2)\n",
    "    - phrases.Phrase[5]=='series'\n",
    "    - all_text[7]=='of'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('/home/greyatom/Desktop/sentiment analysis/')\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "\n",
    "# Code starts here\n",
    "\n",
    "# Load the training data\n",
    "df_train = pd.DataFrame.from_csv(\"train.tsv\", sep=\"\\t\")\n",
    "\n",
    "# Load the testing data\n",
    "df_test = pd.DataFrame.from_csv(\"test.tsv\", sep=\"\\t\")\n",
    "\n",
    "# Concat the 'Phrases' column from train and test dataset\n",
    "phrases = pd.concat([df_train[[\"Phrase\"]], df_test[[\"Phrase\"]]])\n",
    "\n",
    "# Shape of the new dataframe\n",
    "print (phrases.shape[0]==(df_train.shape[0]+df_test.shape[0]))\n",
    "\n",
    "# Convert all the phrases to lower case\n",
    "all_text = phrases[\"Phrase\"].str.lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PhraseId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>A series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          SentenceId                                             Phrase  \\\n",
       "PhraseId                                                                  \n",
       "1                  1  A series of escapades demonstrating the adage ...   \n",
       "2                  1  A series of escapades demonstrating the adage ...   \n",
       "3                  1                                           A series   \n",
       "4                  1                                                  A   \n",
       "5                  1                                             series   \n",
       "\n",
       "          Sentiment  \n",
       "PhraseId             \n",
       "1                 1  \n",
       "2                 2  \n",
       "3                 2  \n",
       "4                 2  \n",
       "5                 2  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Vectorizer\n",
    "Apart from Count vectorizer an alternative to calculate word frequencies , and by far the most popular method is called TF-IDF. This is an acronym than stands for “Term Frequency – Inverse Document” Frequency which are the components of the resulting scores assigned to each word.\n",
    "\n",
    "    * Term Frequency: This summarizes how often a given word appears within a document.\n",
    "    * Inverse Document Frequency: This downscales words that appear a lot across documents.\n",
    "\n",
    "TF-IDF are word frequency scores that try to highlight words that are more interesting, e.g. frequent in a document but not across documents.\n",
    "\n",
    "## Instructions\n",
    "* Instantiate `TfidfVectorizer(stop_words=\"english\")` and store it in a variable `tfidf`\n",
    "\n",
    "* Fit `tfidf` on `all_text` data\n",
    "\n",
    "* Transform `all_text` data using `tfidf` and store it in a variable `X`\n",
    "\n",
    "* Store data upto index 156060 in a variable `X_train` and the data after index 156060 in a variable `X_test`\n",
    "\n",
    "* Store the `Sentiment` column from `df_train` in a variable `y`\n",
    "\n",
    "* Split the train data into `X_train_train`, `X_train_test`,`y_train_train` and `y_train_test` and pass the parameters as `X_train`,`y`,`test_size`=0.3 and `random_state = 42`\n",
    "\n",
    "## Hints\n",
    "* Use `tfidf.transform(all_text).toarray()` to convert the vectorizer result to an array\n",
    "\n",
    "* Use `X_train_train, X_train_test, y_train_train, y_train_test = tts(X_train,y,test_size=0.3, random_state=42)` to split the training data into train and test part.\n",
    "\n",
    "## Test case\n",
    "\n",
    "Variable declaration `tfidf`,`X`,`y`,`X_train_train`,`X_train_test`,`y_train_train`,`y_train_test`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Code starts here\n",
    "# TF-IDF vectorize all text after removing the 'stopwords'\n",
    "tfidf = TfidfVectorizer(stop_words=\"english\")\n",
    "tfidf.fit(all_text)\n",
    "\n",
    "# Vectorizer result to an array\n",
    "X = tfidf.transform(all_text)\n",
    "# Train and test data\n",
    "X_train= X[:156060]\n",
    "X_test = X[156060:]\n",
    "\n",
    "# New column 'y' assigned with the `sentiment` column from training data\n",
    "y = df_train[\"Sentiment\"]\n",
    "\n",
    "# X_train and y shape match\n",
    "print (X_train.shape[0]==y.shape[0])\n",
    "\n",
    "# Dividing the train data into training and testing data\n",
    "X_train_train, X_train_test, y_train_train, y_train_test = train_test_split(X_train,y,test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes classsifier\n",
    "\n",
    "A Naive Bayes classifier is a probabilistic machine learning model that’s used for classification task. The crux of the classifier is based on the Bayes theorem. Using Bayes theorem, we can find the probability of A happening, given that B has occurred. Here, B is the evidence and A is the hypothesis. The assumption made here is that the predictors/features are independent. That is presence of one particular feature does not affect the other. Hence it is called naive. We will use Naive Bayes classifier and see how it performs on our data.\n",
    "\n",
    "## Instructions\n",
    "* Instantiate the `MultinomialNB()` into a variable `nb`\n",
    "\n",
    "* Fit `nb` on `X_train_train` and `y_train_train`\n",
    "\n",
    "* Store the values predicted by `nb` on `X_train_test` in a variable `y_pred`\n",
    "\n",
    "* Find the accuracy score using `accuracy_score()` and store it in a variable `nb_accuracy`\n",
    "\n",
    "## Hints\n",
    "* Use `nb.fit(X_train_train,y_train_train)` to fit the model on train data\n",
    "\n",
    "## Test case\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Code starts here\n",
    "\n",
    "# Instantiate NB classifier\n",
    "nb = MultinomialNB()\n",
    "\n",
    "# Fit the NB classifier on train and test data\n",
    "nb.fit(X_train_train,y_train_train)\n",
    "\n",
    "# Predictions of the test data\n",
    "y_pred = nb.predict(X_train_test)\n",
    "\n",
    "# Accuracy of the model\n",
    "nb_accuracy = accuracy_score(y_train_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count vectorizer with Support vector classifier and Naive bayes algorithm\n",
    "\n",
    "We will here fit the data on Count vectorizer which counts the word frequencies and use this data on two machine learning models 'Support vector classifier' and 'Naive bayes'. We will check which of the two models gives a better performance and use that model on our final test data to get the predictions.\n",
    "\n",
    "\n",
    "## Instructions\n",
    "\n",
    "* Instantiate `CountVectorizer(stop_words=\"english\")` to a variable `cv`\n",
    "\n",
    "* Fit Count vectorizer i.e `cv` on `all_text` data\n",
    "\n",
    "* Transform `all_text` data using `cv`, convert the data into an array amnd store it in a variable `X`\n",
    "\n",
    "* Store data upto index 156060 in a variable `X_train` and the data after index 156060 in a variable `X_test`\n",
    "\n",
    "* Split the train data into `X_train_train`, `X_train_test`,`y_train_train` and `y_train_test` and pass the parameters as `X_train`,`y`,`test_size`=0.3 and `random_state = 42`\n",
    "\n",
    "* Fit `nb` i.e the Naive Bayes classifier model on `X_train_train` and `y_train_train`\n",
    "\n",
    "* Store the predicted values by `nb` model on `X_train_test` and store in a variable `y_pred`\n",
    "\n",
    "* Store the accuracy score using `accuracy_score()` in a variable `nb_accuracy` and print it as well\n",
    "\n",
    "* Instantiate the Support vector classifier model `LinearSVC()` and store it in a variable `svc`\n",
    "\n",
    "* Fit `svc` model on `X_train_train` and `y_train_train` and store it in a variable `model`\n",
    "\n",
    "* Store the predicted values by `model` on `X_train_test` in a variable `y_pred`\n",
    "\n",
    "* Store the accuracy score using `accuracy_score()` in a variable `svc_accuracy` and print it as well\n",
    "\n",
    "## Hints\n",
    "\n",
    "* Use `X_train_train, X_train_test, y_train_train, y_train_test = tts(X_train,y,test_size=0.3, random_state=42)` to split the data and store it in the respective variables\n",
    "\n",
    "* Use `svc.fit(X_train_train,y_train_train)` to fit the `svc` model on training data\n",
    "\n",
    "## Test case\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "\n",
    "# Code starts here\n",
    "\n",
    "# Instantiate Count vectorizer\n",
    "cv = CountVectorizer(stop_words=\"english\")\n",
    "\n",
    "# Fit the data on count vectorizer\n",
    "cv.fit(all_text)\n",
    "\n",
    "# Vectorizer result to an array\n",
    "X = cv.transform(all_text)\n",
    "\n",
    "# Train and test data\n",
    "X_train= X[:156060]\n",
    "X_test = X[156060:]\n",
    "print (X_train.shape[0]==y.shape[0])\n",
    "\n",
    "# Split the data into train and test data\n",
    "X_train_train, X_train_test, y_train_train, y_train_test = train_test_split(X_train,y,test_size=0.3, random_state=42)\n",
    "\n",
    "# Fit NB classifier on train data\n",
    "nb.fit(X_train_train,y_train_train)\n",
    "\n",
    "# Predicted result of test data using NB classifier\n",
    "y_pred_nb = nb.predict(X_train_test)\n",
    "\n",
    "# Accuracy of model\n",
    "nb_accuracy = accuracy_score(y_train_test,y_pred_nb)\n",
    "\n",
    "# Instantiate the LinearSVC() model\n",
    "svc = LinearSVC()\n",
    "\n",
    "# Fit svc classifier on train data\n",
    "model = svc.fit(X_train_train,y_train_train)\n",
    "\n",
    "# Predicted result of test data using svc\n",
    "y_pred_svc = model.predict(X_train_test)\n",
    "\n",
    "# Accuracy model\n",
    "svc_accuracy = accuracy_score(y_train_test,y_pred_svc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the best model on test data\n",
    "\n",
    "As seen in the previous tasks that we preprocessed and worked only on our training data so that our test data is not exposed to the final model any how. In the previous task we saw that Linear SVC with Count Vectorizer gives the best accuracy. We will run this particular model on the test data and get the output\n",
    "\n",
    "## Instructions\n",
    "* Store the predicted values by `model` on `X_test` and in a variable `y_pred`\n",
    "\n",
    "* Append a new column `predicted_sentiment` to the test data `df_test` data\n",
    "\n",
    "## Note : \n",
    "A code snippet to convert your predicted dataframe to an excel sheet is shared with you. This will be helpful during hackathons and competetions.\n",
    "```python\n",
    "writer = pd.ExcelWriter('test_output.xlsx')\n",
    "df_test.to_excel(writer,'Sheet1')\n",
    "\n",
    "writer.save()\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## Hints\n",
    "* Use `df_test[\"predicted_sentiment\"] = y_pred` to store the predicted values in a new column `predicted_sentiment`\n",
    "\n",
    "## Test case\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the best model on the test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Append a column 'predicted_sentiment' to the test data\n",
    "df_test[\"predicted_sentiment\"] = y_pred\n",
    "\n",
    "# Convert your dataframe into an excel sheet and save it\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## END OF NOTEBOOK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
