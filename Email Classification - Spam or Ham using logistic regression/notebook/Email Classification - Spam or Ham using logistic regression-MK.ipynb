{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"margin-block-start: 1em; margin-block-end: 1em; margin-inline-start: 0px; margin-inline-end: 0px; vertical-align: middle; text-align: center;  border: 3px solid green; \">\n",
    "    <a target=\"_top\" href=\"https://colab.research.google.com/github/maksteel/GLabs_Data_Science_Learn/blob/master/Email Classification - Spam or Ham using logistic regression/notebook/Email Classification - Spam or Ham using logistic regression-MK.ipynb\">\n",
    "      <img style=\"height: 4rem; padding-right: 0.5rem; vertical-align: middle;\" src=\"https://colab.research.google.com/img/colab_favicon.ico\">\n",
    "      <strong style=\"font-size: 2em; vertical-align: middle;\">View in Colab</strong>\n",
    "    </a>\n",
    "  </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources used in the session:\n",
    "\n",
    "- [Wiki Confusion Matrix](https://en.wikipedia.org/wiki/Confusion_matrix)\n",
    "- [XKCD Machine Learning](https://xkcd.com/1838/)\n",
    "- [Tuning Hyper Parameters](https://scikit-learn.org/stable/modules/grid_search.html#exhaustive-grid-search)\n",
    "- [Model Specific Cross Validation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegressionCV.html#sklearn.linear_model.LogisticRegressionCV)\n",
    "- [Increase accuracy of Logistic regression](https://stackoverflow.com/questions/38077190/how-to-increase-the-model-accuracy-of-logistic-regression-in-scikit-python)\n",
    "- [Order of feature/model selection and parameter tuning](https://stats.stackexchange.com/questions/264533/how-should-feature-selection-and-hyperparameter-optimization-be-ordered-in-the-m)\n",
    "- Scikit-Learn Pipeline [[1]](https://scikit-learn.org/stable/tutorial/statistical_inference/putting_together.html) [[2]](https://scikit-learn.org/stable/auto_examples/compose/plot_compare_reduction.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Classifying Email as Spam or Non-Spam\n",
    "\n",
    "[Source: UCI ML Repo: Spambase Data Set](https://archive.ics.uci.edu/ml/datasets/spambase) \n",
    "\n",
    "Our collection of spam e-mails came from our postmaster and individuals who had filed spam. Our collection of non-spam e-mails came from filed work and personal e-mails, and hence the word 'george' and the area code '650' are indicators of non-spam. These are useful when constructing a personalized spam filter. One would either have to blind such non-spam indicators or get a very wide collection of non-spam to generate a general purpose spam filter. \n",
    "\n",
    "-  Number of Instances: 4601 (1813 Spam = 39.4%)\n",
    "-  Number of Attributes: 58 (57 continuous, 1 nominal class label)\n",
    "\n",
    "-  Attribute Information:\n",
    "\n",
    "    -  The last column of 'spambase.data' denotes whether the e-mail was \n",
    "    considered spam (1) or not (0)\n",
    "    \n",
    "    - 48 attributes are continuous real [0,100] numbers of type `word freq WORD` i.e. percentage of words in the e-mail that match WORD\n",
    "\n",
    "    - 6 attributes are continuous real [0,100] numbers of type `char freq CHAR` i.e. percentage of characters in the e-mail that match CHAR\n",
    "\n",
    "    - 1 attribute is continuous real [1,...] numbers of type `capital run length average` i.e. average length of uninterrupted sequences of capital letters\n",
    "\n",
    "    - 1 attribute is continuous integer \\[1,...\\] numbers of type\n",
    "`capital run length longest` i.e. length of longest uninterrupted sequence of capital letters\n",
    "\n",
    "    - 1 attribute is continuous integer \\[1,...\\] numbers of type `capital run length total` i.e.\n",
    "sum of length of uninterrupted sequences of capital letters in the email\n",
    "\n",
    "    - 1 attribute is nominal {0,1} class  of type spam i.e  denotes whether the e-mail was considered spam (1) or not (0),  \n",
    "\n",
    "- Missing Attribute Values: None\n",
    "\n",
    "- Class Distribution: \n",
    "|   |   |\n",
    "|---|---|\n",
    "| Spam | 1813  (39.4%) |\n",
    "| Non-Spam  | 2788  (60.6%) |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part A: Base Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### Task 1: Load the data  stored in `path` using `.read_csv()` api."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.756</td>\n",
       "      <td>61</td>\n",
       "      <td>278</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.21</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.94</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.048</td>\n",
       "      <td>5.114</td>\n",
       "      <td>101</td>\n",
       "      <td>1028</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.010</td>\n",
       "      <td>9.821</td>\n",
       "      <td>485</td>\n",
       "      <td>2259</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0     1     2    3     4     5     6     7     8     9   ...    48  \\\n",
       "0  0.00  0.64  0.64  0.0  0.32  0.00  0.00  0.00  0.00  0.00  ...  0.00   \n",
       "1  0.21  0.28  0.50  0.0  0.14  0.28  0.21  0.07  0.00  0.94  ...  0.00   \n",
       "2  0.06  0.00  0.71  0.0  1.23  0.19  0.19  0.12  0.64  0.25  ...  0.01   \n",
       "3  0.00  0.00  0.00  0.0  0.63  0.00  0.31  0.63  0.31  0.63  ...  0.00   \n",
       "4  0.00  0.00  0.00  0.0  0.63  0.00  0.31  0.63  0.31  0.63  ...  0.00   \n",
       "\n",
       "      49   50     51     52     53     54   55    56  57  \n",
       "0  0.000  0.0  0.778  0.000  0.000  3.756   61   278   1  \n",
       "1  0.132  0.0  0.372  0.180  0.048  5.114  101  1028   1  \n",
       "2  0.143  0.0  0.276  0.184  0.010  9.821  485  2259   1  \n",
       "3  0.137  0.0  0.137  0.000  0.000  3.537   40   191   1  \n",
       "4  0.135  0.0  0.135  0.000  0.000  3.537   40   191   1  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/email_data.csv', header=None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insight: With no headers we won't know which `WORDs` or `CHARs` are used in the dataset. However, this does not stop us from using an ML algorithm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### Task 2: Get an overview of your data by using info() and describe() functions of pandas.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4601 entries, 0 to 4600\n",
      "Data columns (total 58 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   0       4601 non-null   float64\n",
      " 1   1       4601 non-null   float64\n",
      " 2   2       4601 non-null   float64\n",
      " 3   3       4601 non-null   float64\n",
      " 4   4       4601 non-null   float64\n",
      " 5   5       4601 non-null   float64\n",
      " 6   6       4601 non-null   float64\n",
      " 7   7       4601 non-null   float64\n",
      " 8   8       4601 non-null   float64\n",
      " 9   9       4601 non-null   float64\n",
      " 10  10      4601 non-null   float64\n",
      " 11  11      4601 non-null   float64\n",
      " 12  12      4601 non-null   float64\n",
      " 13  13      4601 non-null   float64\n",
      " 14  14      4601 non-null   float64\n",
      " 15  15      4601 non-null   float64\n",
      " 16  16      4601 non-null   float64\n",
      " 17  17      4601 non-null   float64\n",
      " 18  18      4601 non-null   float64\n",
      " 19  19      4601 non-null   float64\n",
      " 20  20      4601 non-null   float64\n",
      " 21  21      4601 non-null   float64\n",
      " 22  22      4601 non-null   float64\n",
      " 23  23      4601 non-null   float64\n",
      " 24  24      4601 non-null   float64\n",
      " 25  25      4601 non-null   float64\n",
      " 26  26      4601 non-null   float64\n",
      " 27  27      4601 non-null   float64\n",
      " 28  28      4601 non-null   float64\n",
      " 29  29      4601 non-null   float64\n",
      " 30  30      4601 non-null   float64\n",
      " 31  31      4601 non-null   float64\n",
      " 32  32      4601 non-null   float64\n",
      " 33  33      4601 non-null   float64\n",
      " 34  34      4601 non-null   float64\n",
      " 35  35      4601 non-null   float64\n",
      " 36  36      4601 non-null   float64\n",
      " 37  37      4601 non-null   float64\n",
      " 38  38      4601 non-null   float64\n",
      " 39  39      4601 non-null   float64\n",
      " 40  40      4601 non-null   float64\n",
      " 41  41      4601 non-null   float64\n",
      " 42  42      4601 non-null   float64\n",
      " 43  43      4601 non-null   float64\n",
      " 44  44      4601 non-null   float64\n",
      " 45  45      4601 non-null   float64\n",
      " 46  46      4601 non-null   float64\n",
      " 47  47      4601 non-null   float64\n",
      " 48  48      4601 non-null   float64\n",
      " 49  49      4601 non-null   float64\n",
      " 50  50      4601 non-null   float64\n",
      " 51  51      4601 non-null   float64\n",
      " 52  52      4601 non-null   float64\n",
      " 53  53      4601 non-null   float64\n",
      " 54  54      4601 non-null   float64\n",
      " 55  55      4601 non-null   int64  \n",
      " 56  56      4601 non-null   int64  \n",
      " 57  57      4601 non-null   int64  \n",
      "dtypes: float64(55), int64(3)\n",
      "memory usage: 2.0 MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.104553</td>\n",
       "      <td>0.213015</td>\n",
       "      <td>0.280656</td>\n",
       "      <td>0.065425</td>\n",
       "      <td>0.312223</td>\n",
       "      <td>0.095901</td>\n",
       "      <td>0.114208</td>\n",
       "      <td>0.105295</td>\n",
       "      <td>0.090067</td>\n",
       "      <td>0.239413</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038575</td>\n",
       "      <td>0.139030</td>\n",
       "      <td>0.016976</td>\n",
       "      <td>0.269071</td>\n",
       "      <td>0.075811</td>\n",
       "      <td>0.044238</td>\n",
       "      <td>5.191515</td>\n",
       "      <td>52.172789</td>\n",
       "      <td>283.289285</td>\n",
       "      <td>0.394045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.305358</td>\n",
       "      <td>1.290575</td>\n",
       "      <td>0.504143</td>\n",
       "      <td>1.395151</td>\n",
       "      <td>0.672513</td>\n",
       "      <td>0.273824</td>\n",
       "      <td>0.391441</td>\n",
       "      <td>0.401071</td>\n",
       "      <td>0.278616</td>\n",
       "      <td>0.644755</td>\n",
       "      <td>...</td>\n",
       "      <td>0.243471</td>\n",
       "      <td>0.270355</td>\n",
       "      <td>0.109394</td>\n",
       "      <td>0.815672</td>\n",
       "      <td>0.245882</td>\n",
       "      <td>0.429342</td>\n",
       "      <td>31.729449</td>\n",
       "      <td>194.891310</td>\n",
       "      <td>606.347851</td>\n",
       "      <td>0.488698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.588000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.065000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.276000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>95.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.380000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.188000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.315000</td>\n",
       "      <td>0.052000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.706000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>266.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4.540000</td>\n",
       "      <td>14.280000</td>\n",
       "      <td>5.100000</td>\n",
       "      <td>42.810000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>5.880000</td>\n",
       "      <td>7.270000</td>\n",
       "      <td>11.110000</td>\n",
       "      <td>5.260000</td>\n",
       "      <td>18.180000</td>\n",
       "      <td>...</td>\n",
       "      <td>4.385000</td>\n",
       "      <td>9.752000</td>\n",
       "      <td>4.081000</td>\n",
       "      <td>32.478000</td>\n",
       "      <td>6.003000</td>\n",
       "      <td>19.829000</td>\n",
       "      <td>1102.500000</td>\n",
       "      <td>9989.000000</td>\n",
       "      <td>15841.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                0            1            2            3            4   \\\n",
       "count  4601.000000  4601.000000  4601.000000  4601.000000  4601.000000   \n",
       "mean      0.104553     0.213015     0.280656     0.065425     0.312223   \n",
       "std       0.305358     1.290575     0.504143     1.395151     0.672513   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "50%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "75%       0.000000     0.000000     0.420000     0.000000     0.380000   \n",
       "max       4.540000    14.280000     5.100000    42.810000    10.000000   \n",
       "\n",
       "                5            6            7            8            9   ...  \\\n",
       "count  4601.000000  4601.000000  4601.000000  4601.000000  4601.000000  ...   \n",
       "mean      0.095901     0.114208     0.105295     0.090067     0.239413  ...   \n",
       "std       0.273824     0.391441     0.401071     0.278616     0.644755  ...   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
       "50%       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
       "75%       0.000000     0.000000     0.000000     0.000000     0.160000  ...   \n",
       "max       5.880000     7.270000    11.110000     5.260000    18.180000  ...   \n",
       "\n",
       "                48           49           50           51           52  \\\n",
       "count  4601.000000  4601.000000  4601.000000  4601.000000  4601.000000   \n",
       "mean      0.038575     0.139030     0.016976     0.269071     0.075811   \n",
       "std       0.243471     0.270355     0.109394     0.815672     0.245882   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "50%       0.000000     0.065000     0.000000     0.000000     0.000000   \n",
       "75%       0.000000     0.188000     0.000000     0.315000     0.052000   \n",
       "max       4.385000     9.752000     4.081000    32.478000     6.003000   \n",
       "\n",
       "                53           54           55            56           57  \n",
       "count  4601.000000  4601.000000  4601.000000   4601.000000  4601.000000  \n",
       "mean      0.044238     5.191515    52.172789    283.289285     0.394045  \n",
       "std       0.429342    31.729449   194.891310    606.347851     0.488698  \n",
       "min       0.000000     1.000000     1.000000      1.000000     0.000000  \n",
       "25%       0.000000     1.588000     6.000000     35.000000     0.000000  \n",
       "50%       0.000000     2.276000    15.000000     95.000000     0.000000  \n",
       "75%       0.000000     3.706000    43.000000    266.000000     1.000000  \n",
       "max      19.829000  1102.500000  9989.000000  15841.000000     1.000000  \n",
       "\n",
       "[8 rows x 58 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.info()\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insight: Data is clean and all features are numeric. Also many word frequencies appear to be zero (looking at their quartile values), indicating these words maybe the ones that help decide spam from not-spam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Task 3.1: Split the data into train and test set and fit the base logistic regression model on train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(random_state=101)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df.iloc[:,:-1]\n",
    "y = df.iloc[:,-1]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y , test_size= 0.3, random_state = 42)\n",
    "lr = LogisticRegression(random_state=101)\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.2 Compare predicted values and observed values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for 10 observation:     [0 0 0 0 0 1 0 0 0 0]\n",
      "Actual values for 10 observation:  [0 0 0 1 0 1 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Compare observed value and Predicted value\n",
    "print(\"Prediction for 10 observation:    \",lr.predict(X_test[0:10]))\n",
    "print(\"Actual values for 10 observation: \",y_test[0:10].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insight: Fantastic, 9/10 are correct predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.3 Find out the accuracy, print out the Classification report and Confusion Matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test data: 0.9210716871832005\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy on test data:\", lr.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: \n",
      " [[750  54]\n",
      " [ 55 522]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7fa9b05e8e48>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAATgAAAEGCAYAAADxD4m3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAb4klEQVR4nO3de5hdVZ3m8e+bSuXGJfeE3DAIIYi2hBAxLTOKxEZCo8F+yAiopJExwiBesGVAnVa6vWD7jHSjDk40SHAEpNU0GUUQI7TgSCBgjEKAhDQkMTeSkBDIveo3f+xVcAhVp84m5+Scs+v9PM9+au911ll7Verhx1p77bWWIgIzsyLqVe8KmJnVigOcmRWWA5yZFZYDnJkVlgOcmRVW73pXoNSwIS0xflxrvathOTy5dEC9q2A57OJF9sRuHUgZ737nIbF5S1tFeR9euvuuiDjjQO53IBoqwI0f18qDd42rdzUsh3ePnlTvKlgOi2LhAZexeUsbD951ZEV5W0YtH3bANzwADRXgzKzxBdBOe72rUREHODPLJQj2RmVd1HpzgDOz3NyCM7NCCoK2Jpni6QBnZrm14wBnZgUUQJsDnJkVlVtwZlZIAez1MzgzK6Ig3EU1s4IKaGuO+OYAZ2b5ZDMZmoNXEzGznERbhUfZUqSJkpaUHM9L+qSkIZLulrQ8/Ryc8kvSdZJWSFoqaXJ3NXWAM7NcskEGVXSULSfiiYiYFBGTgJOAHcB84EpgYURMABama4DpwIR0zAau766uDnBmlkv2HtyBt+D2Mw14KiKeAWYA81L6PODsdD4DuCkyDwCDJI0qV6ifwZlZbu3dtM5KDJO0uOR6TkTM6STfucAt6XxkRKwDiIh1kkak9DHA6pLvrElp67q6uQOcmeXS0YKr0KaImFIug6Q+wHuBq7opq7Oblh3PdYAzs1wC0Vbdp1vTgUciYkO63iBpVGq9jQI2pvQ1QOmKuGOBteUK9jM4M8utPVTRUaHzeLl7CrAAmJXOZwG3l6RfkEZTpwLbOrqyXXELzsxyCcSeaKlKWZIGAH8FfLQk+RrgNkkXAauAmSn9DuBMYAXZiOuF3ZXvAGdmuWQv+lan8xcRO4Ch+6VtJhtV3T9vAJfmKd8Bzsxyy/kKSN04wJlZLhGiLZrj8b0DnJnl1u4WnJkVUTbI0ByhozlqaWYNo5qDDLXmAGdmubVV/o5bXTnAmVkuNZjJUDMOcGaWW7tHUc2siLLJ9g5wZlZAgdhbpalateYAZ2a5ROAXfc2sqOQXfc2smAK34MyswDzIYGaFFORazLKuHODMLJds28DmCB3NUUszayC5twSsGwc4M8sl8EwGMyswt+DMrJAi5BacmRVTNsjgqVpmVkjNsydDc9TSzBpGNshQnY2fJQ2S9GNJj0taJukvJQ2RdLek5enn4JRXkq6TtELSUkmTuyvfAc7McmujV0VHBf4FuDMijgNOAJYBVwILI2ICsDBdA0wHJqRjNnB9d4U7wJlZLh0zGQ60BSfpcODtwFyAiNgTEVuBGcC8lG0ecHY6nwHcFJkHgEGSRpW7hwOcmeXWTq+KDmCYpMUlx+ySYl4PPAt8X9LvJX1P0iHAyIhYB5B+jkj5xwCrS76/JqV1yYMMZpZLBOxtr7httCkipnTxWW9gMnBZRCyS9C+83B3tTGdNwih3c7fgzCyXrIvaq6KjG2uANRGxKF3/mCzgbejoeqafG0vyjyv5/lhgbbkbOMCZWW5taT5qd0c5EbEeWC1pYkqaBjwGLABmpbRZwO3pfAFwQRpNnQps6+jKdsVd1AO0ekVfvnLx+Jeu16/qw4c+s54Xt7Xwi5uHMHBIGwAXXrWWk6dtB+DWb47gzluG0tIruORLf2bKqdvrUXVL5i16jJ0vtNDeDm37xGXTj33ps3Mu3shH/n4dM9/0Rp7f4v9c4OXXRKrkMuCHkvoAK4ELyRpet0m6CFgFzEx57wDOBFYAO1Lesmr6F5N0BtkwcAvwvYi4ppb3q4dxx+zm+l89AUBbG3xg8hs5ZfpWfnnrUN73kWeZecmzr8j/zJN9uff2wcy553G2bGjlyvcfzdz7l9HSHC+GF9YVM49+VQAbPnoPJ759OxvWtNapVo2qelO1ImIJ0Nkzummd5A3g0jzl16yLKqkF+DbZuyvHA+dJOr5W92sES+47jFGv283IsXu7zPO7uwZy6ozn6NM3OOLIPYwev5snfj/gINbSKvXRL65l7pdGE2UfY/dM7Wlfhu6OeqvlM7iTgRURsTIi9gC3kr3HUlj33j6IU8/e+tL1//3+cC6eNpH/+alxbN+aNdE2rWtl+OiXA+CwUXvZvN4thLoK8ZVbVvKtO59k+gc2AzD19G1sWt/Kysf617lyjScbRW2p6Ki3WnZRO3tn5a37Z0rvxcwGOHJM8z7j2LtHPPDLgXz4s9kzz7NmbeL8T61Hgnn/dARzrh7Np69d3fmgdv3/R9ejfWrGMWzZ0MrAoXu55taVrF7Rl/M+vpGrznt9vavWkJppyfJatuAqemclIuZExJSImDJ8aP0j/mv10K8P45i/2MHg4fsAGDx8Hy0t0KsXTP/AFp5YknVDh43ey7NrX26xbVrXytCRXXdprfa2bMj+Hts2t/LbOwfy5r98kSOO3MP1v3qCeYseY/iovXz7ricZPNx/pw7uor6Gd1aa2b3/NvgV3dPNG15ujf6/Xwxk/MRdAEw9/XnuvX0we3aL9av68Of/6MvEE3cc9Ppapm//Nvof0vbS+Unv2M6TS/rz/je/kVlvPZ5Zbz2eZ9e1cum7j+W5Z/0oAao72b7WatknfAiYIOko4M/AucD5Nbxf3ezaIR657zA+8U8v98jnfmk0Tz3aHwlGjt3Dx9Nn4yfu4u3v2crsU4+jpSX42FfWeAS1jgYP38cX5j4NQEvv4J75g1l87+H1rVQTaJYFLxU1HCKSdCbwz2SvidwQEV8ul3/KCf3iwbvGlctiDebdoyfVuwqWw6JYyPOx5YCaVoOPGxGn3XBORXl/esr1D5eZqlVzNX2qHxF3kL2cZ2YF0gjdz0o077ClmdVFlWcy1JQDnJnl5gBnZoXUTO/BOcCZWW6N8I5bJRzgzCyXCNhX+YKXdeUAZ2a5uYtqZoXkZ3BmVmjhAGdmReVBBjMrpAg/gzOzwhJtHkU1s6LyMzgzKyTPRTWz4gqaZiOe5uhIm1lDqdaS5ZKelvRHSUskLU5pQyTdLWl5+jk4pUvSdZJWSFoqaXJ35TvAmVkukQYZKjkq9M6ImFSyMOaVwMKImAAsTNeQbUE6IR2zgeu7K9gBzsxyi6jseI1mAPPS+Tzg7JL0myLzADBI0qhyBTnAmVluEaroAIZJWlxyzN6/KOCXkh4u+WxkRKzL7hPrgBEpvbOtSMeUq6cHGcwsl6x1VvEo6qZu9mQ4JSLWShoB3C3p8TJ5K9qKtJQDnJnlVq3XRCJibfq5UdJ84GRgg6RREbEudUE3puy5tyJ1F9XMcqvGMzhJh0g6rOMcOB34E7AAmJWyzQJuT+cLgAvSaOpUYFtHV7YrbsGZWS6BaK/OVK2RwHxJkMWimyPiTkkPAbdJughYBcxM+e8AzgRWADuAC7u7gQOcmeVWjfd8I2IlcEIn6ZuBaZ2kB3Bpnns4wJlZPvkGGerKAc7M8muSqVpdBjhJh5f7YkQ8X/3qmFkzKEIL7lGyOF36m3RcB3BkDetlZg0qgPb2Jg9wETGuq8/MrAcLoElacBWN9Uo6V9Jn0/lYSSfVtlpm1shqPBe1aroNcJK+BbwT+FBK2gF8p5aVMrMGFxUedVbJKOrbImKypN8DRMQWSX1qXC8za1gqxCBDh72SepHisaShQHtNa2Vmja0BWmeVqCTAfRv4CTBc0tXAfwGurmmtzKxxBUSzj6J2iIibJD0MvCslzYyIP9W2WmbW2AoS4JIWYC9Zw9QrkJj1dE3SRa1kFPVzwC3AaLL1l26WdFWtK2ZmDaxAo6gfBE6KiB0Akr4MPAx8tZYVM7MG1UQv+lYS4J7ZL19vYGVtqmNmzaARXuKtRLnJ9teSxeodwKOS7krXpwP3H5zqmVlDKsAoasdI6aPAz0vSH6hddcysGajZW3ARMfdgVsTMmkSDDCBUottncJKOBr4MHA/060iPiGNrWC8za1hqmkGGSt5puxH4PtmbfdOB24Bba1gnM2t0TfKaSCUBbkBE3AUQEU9FxOfJVhcxs56qvcKjzip5TWS3sn29npJ0MfBnYERtq2VmDatg78F9CjgU+DjZs7iBwIdrWSkza2xNP4raISIWpdPtvLzopZn1ZFUMcJJagMXAnyPiLElHkT3nHwI8AnwoIvZI6gvcBJwEbAbeHxFPlyu73Iu+8ynza0TE3+T9RczMOvEJYBnQsZPf14BrI+JWSd8BLgKuTz+fi4hjJJ2b8r2/XMHlWnDfOuBq5/Tk0gG8e/Skg31bOwAv3Pn6elfBcmj/2H1VKadaXVRJY4G/Jnv8dXl63n8acH7KMg/4IlmAm5HOAX4MfEuS0o73nSr3ou/CA628mRVQkGeq1jBJi0uu50TEnJLrfwauAA5L10OBrRGxL12vAcak8zHAaoCI2CdpW8q/qaube2d7M8uv8hbcpoiY0tkHks4CNkbEw5JO7Uguc7dyn3XKAc7McqtSF/UU4L2SziSbJXU4WYtukKTeqRU3Flib8q8BxgFrJPUme6NjS7kbVLw6bxrBMDOrykyGiLgqIsZGxHjgXODXEfEB4B7gnJRtFnB7Ol+Qrkmf/7rc8zeobEXfkyX9EVierk+Q9M3uvmdmBVbbqVr/nWzAYQXZM7aOhT/mAkNT+uXAld0VVEkX9TrgLODfACLiD5I8Vcush1JU/0XfiLgXuDedrwRO7iTPLmBmnnIrCXC9IuKZbPT2JW15bmJmBVOABS87rJZ0MhDpjePLgCdrWy0za2SFmaoFXELWTT0S2AD8KqWZWU9VlAAXERvJRjjMzKAGz+BqpZIVfb9LJ/E6ImbXpEZm1viKEuDIuqQd+gHvI02XMLOeSQ2wmGUlKumi/qj0WtIPgLtrViMzsyp5LVO1jgJeV+2KmFkTKUoXVdJzvPzr9CKb+9XtG8RmVlBFGWRIazOdQLYPA0B7d3O/zKwHaJIoUHYuagpm8yOiLR1N8muZWU0VaNvAByVNrnlNzKwpiGwUtZKj3srtydCxHtN/Aj4i6SngRbLfLyLCQc+sJyrIM7gHgcnA2QepLmbWLAoQ4ATZbvYHqS5m1iwKEOCGS7q8qw8j4hs1qI+ZNYEidFFbyHa0b46Fn8zs4ClAgFsXEf9w0GpiZs0hGmOEtBLdPoMzM3uVArTgph20WphZU2n6Z3ARUXa/QTPrwZo9wJmZdapBpmFVouKNn83MIE3VisqOsuVI/SQ9KOkPkh6VdHVKP0rSIknLJf1IUp+U3jddr0ifj++urg5wZpZbNQIcsBs4LSJOACYBZ0iaCnwNuDYiJgDPARel/BcBz0XEMcC1KV9ZDnBmll8VVhOJzAvpsjUdAZwG/Dilz+Pl6aIz0jXp82nab8Pm/TnAmVl+lQe4YZIWlxyv2KxKUoukJcBGsq0QngK2poU+ANYAY9L5GNJ+MOnzbcDQctX0IIOZ5ZNvNZFNETGly6Ii2oBJkgYB84E3dH5HoPN3c8vWxC04M8uvygteRsRW4F5gKjBIUkfjayywNp2vAcZBtpwbMJBsC4UuOcCZWW7VWPBS0vDUckNSf+BdwDLgHuCclG0WcHs6X5CuSZ//urtVxt1FNbPcqjSTYRQwT1ILWWPrtoj4maTHgFslfQn4PTA35Z8L/EDSCrKW27nd3cABzszyqdKLvhGxFDixk/SVwMmdpO8CZua5hwOcmeXXJDMZHODMLJeOmQzNwAHOzHJTe3NEOAc4M8uniSbbO8CZWW7uoppZcTnAmVlRuQVnZsXlAGdmhVSQXbXMzF7F78GZWbGVn+PeMBzgzCw3t+B6qHmLHmPnCy20t0PbPnHZ9GP54KfXM/38zWzbkv1zf/+ro3jo14fXuaY924ALVhEDBL0ELbDzm2Pp893N9F60g+gtYnRvdl0+HA5toeWRHfS5YQvsC+gt9vzXobRN6l/vX6F+/KIvSLoBOAvYGBFvqtV9GtEVM4/m+S2v/Ked/93h/Pg7I+pUI+vMzq+NhoEtL123Te7Png8PgRbRZ+5m+vxoK3suGkoc3sKuq48ghvam19N76Pe5dez44evqWPP6a5ZBhloueHkjcEYNyzerqraTBkBLtip223H90KY2ANqP6UsMzf6H1f66VrQnYE+TNGFqpBoLXh4MNWvBRcRvKtm3sHBCfOWWlRDw8x8M5Rc/zPbEeM+Fm5h2znMsX9qfOVeP5oVtfjpQV4L+n10Hgr1nHs6+M1/5yKD1l9vZ9/ZDXvW1lvtfpO3ovtCn7GZOxRZ4kKFSaZed2QD9GFDn2hy4T804hi0bWhk4dC/X3LqS1Sv68rN5Q7n52pFEwKwr1jP7C2v5xuVH1ruqPdrOb4wmhvZGW9vod9U62se10v4X2XO11luegxbYd9qhr/hOr6f30PeGLez88qh6VLmhNMsgQ933ZIiIORExJSKmtNK33tU5YFs2tAKwbXMrv71zIMeduIOtm1ppbxcR4hc/HMrESTvrXEvr6HLGoBba3jaAlid2A9D77u30XrSDXVeMgJItN/XsPvr94wZ2/d0IYnRrXercUKq86Uyt1D3AFUnf/m30P6TtpfOT3rGdpx/vx5ARe1/K87bp23j6iX71qqIB7GqHHe0vnbc8spP28X1oWbyDPv+6lZ1fPAL6lfyn8UIb/f5+PbsvHEz7G/2363jRtwo729dc3buoRTJ4+D6+MPdpAFp6B/fMH8ziew/nM9et4ug37iQCNqzpw3VXjK1vRXs4PddGv3/YkF20BfveeShtUwYw4MJVsDeyZ3NA+3F92f3x4bQueJ5ea/fS5+atcPNWAHZ9ZRQxqKWrWxRbhBe8lHQLcCrZztZrgC9ExNzy32pu61f15ZK/mviq9K9/3M/bGkmMamXn9a/+n8yO73f+d9p7/mD2nj+41tVqLs0R32o6inperco2s/pqhO5nJfwMzszyCaA9KjvKkDRO0j2Slkl6VNInUvoQSXdLWp5+Dk7pknSdpBWSlkqa3F1VHeDMLL/qjKLuAz4dEW8ApgKXSjoeuBJYGBETgIXpGmA6MCEds4Hru7uBA5yZ5VaNUdSIWBcRj6Tz7cAyYAwwA5iXss0Dzk7nM4CbIvMAMEhS2ZcSPYpqZrnlGEUdJmlxyfWciJjzqvKyWU8nAouAkRGxDrIgKKljEvcYYHXJ19aktHVd3dwBzszyyfcS76aImFIug6RDgZ8An4yI56Uup8F19kHZmriLama5ZC/6RkVHt2VJrWTB7YcR8dOUvKGj65l+bkzpa4BxJV8fC6wtV74DnJnl117hUYayptpcYFlEfKPkowXArHQ+C7i9JP2CNJo6FdjW0ZXtiruoZpZbJa2zCpwCfAj4o6QlKe2zwDXAbZIuAlYBM9NndwBnAiuAHcCF3d3AAc7M8qnSRPqIuJ/On6sBTOskfwCX5rmHA5yZ5eS5qGZWZF7w0swKyRs/m1mhuQVnZoXVHPHNAc7M8lN7c/RRHeDMLJ+g25d4G4UDnJnlIiqbhtUIHODMLD8HODMrLAc4MyskP4MzsyLzKKqZFVS4i2pmBRU4wJlZgTVHD9UBzszy83twZlZcDnBmVkgR0NYcfVQHODPLzy04MyssBzgzK6QAvCeDmRVTQDTHMzhv/Gxm+QTZIEMlRzck3SBpo6Q/laQNkXS3pOXp5+CULknXSVohaamkyd2V7wBnZvlFVHZ070bgjP3SrgQWRsQEYGG6BpgOTEjHbOD67gp3gDOz/KoU4CLiN8CW/ZJnAPPS+Tzg7JL0myLzADBI0qhy5TvAmVlOFQa3LMANk7S45JhdwQ1GRsQ6gPRzREofA6wuybcmpXXJgwxmlk8AlS+XtCkiplTpzuqiNl1yC87M8qveM7jObOjoeqafG1P6GmBcSb6xwNpyBTnAmVlOUbVR1C4sAGal81nA7SXpF6TR1KnAto6ubFfcRTWzfAKiSu/BSboFOJXsWd0a4AvANcBtki4CVgEzU/Y7gDOBFcAO4MLuyneAM7P8qjSTISLO6+KjaZ3kDeDSPOU7wJlZfp6LamaFFJFnFLWuHODMLD+34MysmIJoa6t3JSriAGdm+Xi5JDMrtCZZLskBzsxyCSDcgjOzQormWfDSAc7McmuWQQZFAw33SnoWeKbe9aiBYcCmelfCcinq3+x1ETH8QAqQdCfZv08lNkXE/gtaHjQNFeCKStLiKi4ZYweB/2bF4NVEzKywHODMrLAc4A6OOfWugOXmv1kB+BmcmRWWW3BmVlgOcGZWWA5wNSTpDElPpJ24r+z+G1Zvne20bs3LAa5GJLUA3ybbjft44DxJx9e3VlaBG3n1TuvWpBzgaudkYEVErIyIPcCtZDtzWwPrYqd1a1IOcLWTexduM6suB7jayb0Lt5lVlwNc7eTehdvMqssBrnYeAiZIOkpSH+Bcsp25zewgcYCrkYjYB3wMuAtYBtwWEY/Wt1bWnbTT+u+AiZLWpN3VrUl5qpaZFZZbcGZWWA5wZlZYDnBmVlgOcGZWWA5wZlZYDnBNRFKbpCWS/iTpXyUNOICyTpX0s3T+3nKrnUgaJOm/vYZ7fFHS31Wavl+eGyWdk+Ne470CiO3PAa657IyISRHxJmAPcHHph8rk/ptGxIKIuKZMlkFA7gBnVm8OcM3rPuCY1HJZJul/AY8A4ySdLul3kh5JLb1D4aX16R6XdD/wNx0FSfpbSd9K5yMlzZf0h3S8DbgGODq1Hr+e8n1G0kOSlkq6uqSsz6U18H4FTOzul5D0kVTOHyT9ZL9W6bsk3SfpSUlnpfwtkr5ecu+PHug/pBWXA1wTktSbbJ25P6akicBNEXEi8CLweeBdETEZWAxcLqkf8F3gPcB/Bo7oovjrgH+PiBOAycCjwJXAU6n1+BlJpwMTyJaEmgScJOntkk4im5J2IlkAfUsFv85PI+It6X7LgNKZA+OBdwB/DXwn/Q4XAdsi4i2p/I9IOqqC+1gP1LveFbBc+ktaks7vA+YCo4FnIuKBlD6VbIHN30oC6EM29eg44D8iYjmApP8DzO7kHqcBFwBERBuwTdLg/fKcno7fp+tDyQLeYcD8iNiR7lHJ3Ns3SfoSWTf4ULKpbR1ui4h2YLmklel3OB14c8nzuYHp3k9WcC/rYRzgmsvOiJhUmpCC2IulScDdEXHefvkmUb3lmgR8NSL+9373+ORruMeNwNkR8QdJfwucWvLZ/mVFuvdlEVEaCJE0Pud9rQdwF7V4HgBOkXQMgKQBko4FHgeOknR0yndeF99fCFySvtsi6XBgO1nrrMNdwIdLnu2NkTQC+A3wPkn9JR1G1h3uzmHAOkmtwAf2+2ympF6pzq8Hnkj3viTlR9Kxkg6p4D7WA7kFVzAR8WxqCd0iqW9K/nxEPClpNvBzSZuA+4E3dVLEJ4A5aRWNNuCSiPidpN+m1zB+kZ7DvQH4XWpBvgB8MCIekfQjYAnwDFk3ujv/A1iU8v+RVwbSJ4B/B0YCF0fELknfI3s294iymz8LnF3Zv471NF5NxMwKy11UMyssBzgzKywHODMrLAc4MyssBzgzKywHODMrLAc4Myus/w8m9TWSLPacIQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_pred = lr.predict(X_test)\n",
    "print(\"Confusion Matrix: \\n\",confusion_matrix(y_test,y_pred))\n",
    "\n",
    "## see the plot\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "plot_confusion_matrix(lr, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.93      0.93       804\n",
      "           1       0.91      0.90      0.91       577\n",
      "\n",
      "    accuracy                           0.92      1381\n",
      "   macro avg       0.92      0.92      0.92      1381\n",
      "weighted avg       0.92      0.92      0.92      1381\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Classification Report: \\n\",classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insight: Base model is jus the start, but this time its a pretty good start, lets see if we can improve on it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part B: Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "###  Task 4: Copy dataset df into df1 variable and apply correlation on df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Task 5.1: As we have learned  one of the assumptions of Logistic Regression model is that the independent features should not be correlated to each other (i.e no multicolinearity).\n",
    "\n",
    "So we have to find the features that have a correlation higher that 0.75 and remove the same so that the assumption for logistic regression model is satisfied. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns to be dropped: \n",
      "[33, 39]\n"
     ]
    }
   ],
   "source": [
    "# Remove correlated features \n",
    "## Adapted from \n",
    "## https://chrisalbon.com/machine_learning/feature_selection/drop_highly_correlated_features/\n",
    "corr_matrix = df1.drop(57, axis=1).corr().abs()\n",
    "upper_mask = np.triu(np.ones(corr_matrix.shape),k=1).astype(np.bool)\n",
    "upper = corr_matrix.where(upper_mask)\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.75)]\n",
    "print(\"Columns to be dropped: \")\n",
    "print(to_drop)\n",
    "df1.drop(to_drop,axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5.2: Split the  new subset of the  data acquired by feature selection into train and test set and fit the logistic regression model on train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(random_state=101)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df1.iloc[:,:-1]\n",
    "y = df1.iloc[:,-1]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,random_state = 42)\n",
    "lr = LogisticRegression(random_state=101)\n",
    "lr.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5.3 Find out the accuracy, print out the Classification report and Confusion Matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test data: 0.9210716871832005\n",
      "Confusion Matrix: \n",
      " [[746  58]\n",
      " [ 51 526]]\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.93      0.93       804\n",
      "           1       0.90      0.91      0.91       577\n",
      "\n",
      "    accuracy                           0.92      1381\n",
      "   macro avg       0.92      0.92      0.92      1381\n",
      "weighted avg       0.92      0.92      0.92      1381\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy on test data:\", lr.score(X_test,y_test))\n",
    "y_pred = lr.predict(X_test)\n",
    "print(\"Confusion Matrix: \\n\",confusion_matrix(y_test,y_pred))\n",
    "print(\"Classification Report: \\n\",classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insight: After removing highly correlated features, there is not much change in the score. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Task 6.1: Lets apply a feature selection technique (Chi Squared test) to see whether we can increase our accuracy score. \n",
    "\n",
    "Find the optimum number of features using Chi Square and fit the logistic model on train data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For no of features= 20 , score= 0.9015206372194062\n",
      "For no of features= 25 , score= 0.9102099927588704\n",
      "For no of features= 30 , score= 0.9116582186821144\n",
      "For no of features= 35 , score= 0.9225199131064447\n",
      "For no of features= 40 , score= 0.9210716871832005\n",
      "For no of features= 50 , score= 0.9232440260680667\n",
      "For no of features= 55 , score= 0.9210716871832005\n",
      "High Score is: 0.9232440260680667 with features= 50\n"
     ]
    }
   ],
   "source": [
    "# let us try selecting different number of features using chi2 test\n",
    "nof_list = [20,25,30,35,40,50,55]\n",
    "high_score = 0\n",
    "nof = 0\n",
    "best_chi_model = None\n",
    "best_chi_X_train = None\n",
    "best_chi_X_test = None\n",
    "\n",
    "for n in nof_list:\n",
    "    test = SelectKBest(score_func=chi2 , k= n )\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,random_state = 42)\n",
    "    X_train = test.fit_transform(X_train,y_train)\n",
    "    X_test = test.transform(X_test)\n",
    "    \n",
    "    chi_model = LogisticRegression(random_state=101)\n",
    "    chi_model.fit(X_train,y_train)\n",
    "    print(\"For no of features=\",n,\", score=\", chi_model.score(X_test,y_test))\n",
    "    if chi_model.score(X_test,y_test)>high_score:\n",
    "        high_score = chi_model.score(X_test,y_test)\n",
    "        nof = n \n",
    "        best_chi_model = chi_model\n",
    "        best_chi_X_train = X_train\n",
    "        best_chi_X_test = X_test\n",
    "print(\"High Score is:\",high_score, \"with features=\",nof)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 6.2 Print out the Confusion Matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: \n",
      " [[755  49]\n",
      " [ 57 520]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = best_chi_model.predict(best_chi_X_test)\n",
    "print(\"Confusion Matrix: \\n\",confusion_matrix(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Insight: Using chi squared test there is no or very little change in the score and the optimum features that we got is 50."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown as md\n",
    "md(\"#### Insight: Using chi squared test there is no or very little change in \\\n",
    "the score and the optimum features that we got is {}.\".format(nof))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Task 7.1 Now lets see if we can increase our score using another feature selection technique called Anova.\n",
    "\n",
    "Find the optimum number of features using Anova and fit the logistic model on train data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For no of features= 20 , score= 0.8855901520637219\n",
      "For no of features= 25 , score= 0.9015206372194062\n",
      "For no of features= 30 , score= 0.9174511223750905\n",
      "For no of features= 35 , score= 0.9181752353367125\n",
      "For no of features= 40 , score= 0.9160028964518465\n",
      "For no of features= 50 , score= 0.9246922519913107\n",
      "For no of features= 55 , score= 0.9210716871832005\n",
      "High Score is: 0.9246922519913107 with features= 50\n"
     ]
    }
   ],
   "source": [
    "# let us try selecting different number of features using anova test\n",
    "nof_list = [20,25,30,35,40,50,55]\n",
    "high_score = 0\n",
    "nof = 0\n",
    "best_anova_model = None\n",
    "best_anova_X_train = None\n",
    "best_anova_X_test = None\n",
    "\n",
    "for n in nof_list:\n",
    "    test = SelectKBest(score_func=f_classif , k= n )\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state = 42)\n",
    "    X_train = test.fit_transform(X_train,y_train)\n",
    "    X_test = test.transform(X_test)\n",
    "    anova_model = LogisticRegression()\n",
    "    anova_model.fit(X_train,y_train)\n",
    "    print(\"For no of features=\",n,\", score=\", anova_model.score(X_test,y_test))\n",
    "\n",
    "    if anova_model.score(X_test,y_test)>high_score:\n",
    "        high_score = anova_model.score(X_test,y_test)\n",
    "        nof = n \n",
    "        best_anova_model = anova_model\n",
    "        best_anova_X_train = X_train\n",
    "        best_anova_X_test = X_test\n",
    "print(\"High Score is:\",high_score, \"with features=\",nof)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 7.2 Print out the Confusion Matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: \n",
      " [[754  50]\n",
      " [ 54 523]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = best_anova_model.predict(best_anova_X_test)\n",
    "print(\"Confusion Matrix: \\n\",confusion_matrix(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insight: Number of selected features still seem to remain same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Task 8.1: Let us apply PCA as our last feature selection method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For no of features= 20 , score= 0.9022447501810282\n",
      "For no of features= 25 , score= 0.9044170890658942\n",
      "For no of features= 30 , score= 0.9058653149891384\n",
      "For no of features= 35 , score= 0.9167270094134685\n",
      "For no of features= 40 , score= 0.9196234612599565\n",
      "For no of features= 50 , score= 0.9167270094134685\n",
      "For no of features= 55 , score= 0.9174511223750905\n",
      "High Score is: 0.9196234612599565 with features= 40\n"
     ]
    }
   ],
   "source": [
    "# Apply PCA and fit the logistic model on train data use df dataset\n",
    "nof_list = [20,25,30,35,40,50,55]\n",
    "high_score = 0\n",
    "nof = 0\n",
    "best_pca_lr_model = None\n",
    "best_pca_lr_X_train = None\n",
    "best_pca_lr_X_test = None\n",
    "\n",
    "for n in nof_list:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,random_state = 42)\n",
    "    pca = PCA(n_components=n)\n",
    "    pca.fit(X_train)\n",
    "    X_train = pca.transform(X_train)\n",
    "    X_test = pca.transform(X_test)\n",
    "    pca_lr_model = LogisticRegression(random_state=101)\n",
    "    pca_lr_model.fit(X_train, y_train)\n",
    "    print(\"For no of features=\",n,\", score=\", pca_lr_model.score(X_test,y_test))\n",
    "    \n",
    "    if pca_lr_model.score(X_test,y_test)>high_score:\n",
    "        high_score = pca_lr_model.score(X_test,y_test)\n",
    "        nof = n\n",
    "        best_pca_lr_model = pca_lr_model\n",
    "        best_pca_lr_X_train = X_train\n",
    "        best_pca_lr_X_test = X_test\n",
    "print(\"High Score is:\",high_score, \"with features=\",nof)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 8.2 Print out the Confusion Matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: \n",
      " [[759  45]\n",
      " [ 66 511]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = best_pca_lr_model.predict(best_pca_lr_X_test)\n",
    "print(\"Confusion Matrix: \\n\",confusion_matrix(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insight: There is significant reduction in number of features selected but the score is not the best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part C: Hyper-parameter optimisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Task 9: Let us try to optimise the hyper-parameters of high scoring model with featuers selected with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=LogisticRegression(random_state=101),\n",
       "             param_grid={'penalty': ['l2', 'l1', 'elasticnet'],\n",
       "                         'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag',\n",
       "                                    'saga']})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {\n",
    "    'penalty': ['l2', 'l1', 'elasticnet'],\n",
    "    'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']}\n",
    "search = GridSearchCV(best_pca_lr_model, param_grid, cv=5)\n",
    "search.fit(best_pca_lr_X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'penalty': 'l1', 'solver': 'liblinear'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9170807453416149"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insight: The score did not improve much."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Task 10: Does the order matter? \n",
    "**Method A:**  \n",
    "Try different feature selection techniques, choose the model with best score and finally optimise its hyper-parameters  \n",
    "\n",
    "**Method B:**  \n",
    "Perform feature selection and hyper-parameters tuning for each model, then select the best model\n",
    "\n",
    "So far, we have been trying Method A, let us try Method B, first with Chi-square and Anova, then with all Chi-square, Anova and PCA put together\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'model__penalty': 'l1',\n",
       "  'model__solver': 'liblinear',\n",
       "  'select__k': 55,\n",
       "  'select__score_func': <function sklearn.feature_selection._univariate_selection.f_classif(X, y)>},\n",
       " Pipeline(steps=[('select', SelectKBest(k=55)),\n",
       "                 ('model',\n",
       "                  LogisticRegression(penalty='l1', solver='liblinear'))]),\n",
       " 0.9134954916678467)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using Scikit-learn Pipeline method for Chi2 and Anova\n",
    "nof_list = [20,25,30,35,40,50,55]\n",
    "scoring_func_list = [f_classif, chi2]\n",
    "penalty_list = ['l2', 'l1', 'elasticnet']\n",
    "solver_list = ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('select', SelectKBest()),\n",
    "    ('model', LogisticRegression())\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'select__k': nof_list,\n",
    "    'select__score_func': scoring_func_list,\n",
    "    'model__penalty': penalty_list,\n",
    "    'model__solver': solver_list}\n",
    "\n",
    "search = GridSearchCV(pipe, param_grid, cv=5)\n",
    "best_model = search.fit(X, y)\n",
    "\n",
    "best_model.best_params_,best_model.best_estimator_,best_model.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'model__penalty': 'l1',\n",
       "  'model__solver': 'liblinear',\n",
       "  'select': SelectKBest(k=55, score_func=<function chi2 at 0x7fa9904f97b8>),\n",
       "  'select__k': 55,\n",
       "  'select__score_func': <function sklearn.feature_selection._univariate_selection.chi2(X, y)>},\n",
       " Pipeline(steps=[('select',\n",
       "                  SelectKBest(k=55,\n",
       "                              score_func=<function chi2 at 0x7fa9904f97b8>)),\n",
       "                 ('model',\n",
       "                  LogisticRegression(penalty='l1', solver='liblinear'))]),\n",
       " 0.9139300382382098)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using Scikit-learn Pipeline method for Chi2, Anova and PCA\n",
    "nof_list = [20,25,30,35,40,50,55]\n",
    "scoring_func_list = [f_classif, chi2]\n",
    "penalty_list = ['l2', 'l1', 'elasticnet']\n",
    "solver_list = ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\n",
    "\n",
    "pipe = Pipeline([\n",
    "    # select stage is populated by the param_grid\n",
    "    ('select', 'passthrough'),\n",
    "    ('model', LogisticRegression())\n",
    "])\n",
    "\n",
    "param_grid = [\n",
    "    {\n",
    "        'select':[SelectKBest()],\n",
    "        'select__k': nof_list,\n",
    "        'select__score_func': scoring_func_list,\n",
    "        'model__penalty': penalty_list,\n",
    "        'model__solver': solver_list\n",
    "    },\n",
    "    {\n",
    "        'select':[PCA()],\n",
    "        'select__n_components': nof_list,\n",
    "        'model__penalty': penalty_list,\n",
    "        'model__solver': solver_list\n",
    "    }\n",
    "]\n",
    "search = GridSearchCV(pipe, param_grid, cv=5)\n",
    "best_model = search.fit(X, y)\n",
    "\n",
    "best_model.best_params_,best_model.best_estimator_,best_model.best_score_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
